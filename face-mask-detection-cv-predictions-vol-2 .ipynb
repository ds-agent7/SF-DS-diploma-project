{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Image classification on DL (MobileNetV2). Face mask detection. Predictions at work Vol.2"},{"metadata":{},"cell_type":"markdown","source":"### (Diploma project by student @Pawel_MTW \"Skillfactory.ru\")"},{"metadata":{},"cell_type":"markdown","source":"### В Vol.1 мы построили модель для детектирования маски на лице человека,обучили ее, проверили показатели эффективности, сохранили ее.  Настало время внедрить модель в практическое использование. Этим мы и займемся. "},{"metadata":{},"cell_type":"markdown","source":"## Импорт библиотек/модулей. Основные настройки."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip install cvlib\n!pip install opencv-python\n!pip install xmltodict\n!pip install mtcnn \n!pip install plot-metric\n#!pip install face_recognition\n!pip install --upgrade imutils","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (20.3.3)\nCollecting pip\n  Downloading pip-21.0-py3-none-any.whl (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 1.3 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 20.3.3\n    Uninstalling pip-20.3.3:\n      Successfully uninstalled pip-20.3.3\nSuccessfully installed pip-21.0\nCollecting cvlib\n  Downloading cvlib-0.2.6.tar.gz (10.0 MB)\n\u001b[K     |████████████████████████████████| 10.0 MB 178 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from cvlib) (1.19.5)\nCollecting progressbar\n  Downloading progressbar-2.5.tar.gz (10 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from cvlib) (2.25.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from cvlib) (7.2.0)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (from cvlib) (2.9.0)\nCollecting imutils\n  Downloading imutils-0.5.4.tar.gz (17 kB)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->cvlib) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->cvlib) (1.26.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->cvlib) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->cvlib) (2020.12.5)\nBuilding wheels for collected packages: cvlib, imutils, progressbar\n  Building wheel for cvlib (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for cvlib: filename=cvlib-0.2.6-py3-none-any.whl size=10044621 sha256=1d8972d8b7002234a08c50943372d5ea33dd813d235bc1927ef4b952229fcbea\n  Stored in directory: /root/.cache/pip/wheels/9b/96/9b/373c07517ffe0bedbebc0813aec1e62cb2bb1dee91b3694895\n  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25860 sha256=230f134b4565bffb726d47fe9efc30370bb27b4ae74487ba9be7768a869b3cfb\n  Stored in directory: /root/.cache/pip/wheels/86/d7/0a/4923351ed1cec5d5e24c1eaf8905567b02a0343b24aa873df2\n  Building wheel for progressbar (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12075 sha256=39d339c02da8e6910f85849cedd503e31f0dfe12ac8f381c9d8c15cde1524f36\n  Stored in directory: /root/.cache/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\nSuccessfully built cvlib imutils progressbar\nInstalling collected packages: progressbar, imutils, cvlib\nSuccessfully installed cvlib-0.2.6 imutils-0.5.4 progressbar-2.5\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (4.5.1.48)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-python) (1.19.5)\nCollecting xmltodict\n  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\nInstalling collected packages: xmltodict\nSuccessfully installed xmltodict-0.12.0\nCollecting mtcnn\n  Downloading mtcnn-0.1.0-py3-none-any.whl (2.3 MB)\n\u001b[K     |████████████████████████████████| 2.3 MB 1.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: opencv-python>=4.1.0 in /opt/conda/lib/python3.7/site-packages (from mtcnn) (4.5.1.48)\nRequirement already satisfied: keras>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from mtcnn) (2.4.3)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras>=2.0.0->mtcnn) (2.10.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras>=2.0.0->mtcnn) (5.3.1)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras>=2.0.0->mtcnn) (1.4.1)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras>=2.0.0->mtcnn) (1.19.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras>=2.0.0->mtcnn) (1.15.0)\nInstalling collected packages: mtcnn\nSuccessfully installed mtcnn-0.1.0\nCollecting plot-metric\n  Downloading plot_metric-0.0.6-py3-none-any.whl (13 kB)\nRequirement already satisfied: seaborn>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from plot-metric) (0.11.1)\nRequirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from plot-metric) (1.19.5)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from plot-metric) (1.4.1)\nRequirement already satisfied: matplotlib>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from plot-metric) (3.3.3)\nRequirement already satisfied: pandas>=0.23.4 in /opt/conda/lib/python3.7/site-packages (from plot-metric) (1.2.0)\nRequirement already satisfied: colorlover>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from plot-metric) (0.3.0)\nRequirement already satisfied: scikit-learn>=0.21.2 in /opt/conda/lib/python3.7/site-packages (from plot-metric) (0.23.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.2->plot-metric) (1.3.1)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.2->plot-metric) (7.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.2->plot-metric) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.2->plot-metric) (2.8.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.2->plot-metric) (0.10.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=3.0.2->plot-metric) (1.15.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.4->plot-metric) (2019.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.2->plot-metric) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.2->plot-metric) (1.0.0)\nInstalling collected packages: plot-metric\nSuccessfully installed plot-metric-0.0.6\nRequirement already satisfied: imutils in /opt/conda/lib/python3.7/site-packages (0.5.4)\nCollecting imutils\n  Using cached imutils-0.5.4-py3-none-any.whl\n  Downloading imutils-0.5.3.tar.gz (17 kB)\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import cv2\nimport json\nimport os\nimport argparse\nimport xmltodict\nimport datetime\nimport plot_metric\nimport pylab as pl\nimport cvlib as cv\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport sys, cv2, time\nfrom PIL import Image\nimport tensorflow as tf\nimport scikitplot as skplt\nfrom mtcnn.mtcnn import MTCNN\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nfrom keras.utils import plot_model\nfrom scipy.spatial import distance\nfrom keras.models import Sequential\nfrom warnings import filterwarnings\nfrom keras.models import load_model\nfrom matplotlib.patches import Rectangle\nfrom keras.optimizers import Adam, RMSprop\nfrom sklearn.metrics import roc_auc_score\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom plot_metric.functions import BinaryClassification\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import SpatialDropout2D, BatchNormalization, Input, Activation\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\n\nIMG_SIZE = 124 # какого размера подаем изображения в сеть\nIMG_CHANNELS = 3   # у RGB 3 канала\ninput_shape  = [IMG_SIZE, IMG_SIZE, IMG_CHANNELS]\n\nbatch_size = 32\nMIN_DISTANCE = 80\n\nnum_classes = 2\nlabels=[\"No Mask\",\"Mask\"]\nmask_label = {0:\"No Mask\",1:\"Mask\"}\ncolor_label = {0:(0,0,255),1:(0,255,0),2:(255,0,0),3:(0,0,0),4:(255,255,255)}\ndist_label = {0:(0,255,0),1:(255,0,0)}\n\nDATA_PATH = '../input/'\nPATH = \"../working/\" \nface_model = cv2.CascadeClassifier('../input/haar-cascades-for-face-detection/haarcascade_frontalface_default.xml')\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\n\nfilterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    pass   # print(dirname)\n    #print(os.path.join(dirname, filename))\n    \n#print(os.listdir(DATA_PATH))from keras import models ","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load model. Загружаем обученную модель. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#import tensorflow as tf№\nmodel = tf.keras.models.load_model(\"../input/readymodel3/model_mask (2).hdf5\",\n                                  custom_objects=None, compile=True, options=None)\nmodel.summary()","execution_count":3,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nmobilenetv2_1.40_224 (Functi (None, 4, 4, 1792)        4363712   \n_________________________________________________________________\nglobal_max_pooling2d (Global (None, 1792)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 256)               459008    \n_________________________________________________________________\nactivation_5 (Activation)    (None, 256)               0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 257       \n_________________________________________________________________\nactivation_6 (Activation)    (None, 1)                 0         \n=================================================================\nTotal params: 4,822,977\nTrainable params: 4,775,041\nNon-trainable params: 47,936\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Мы обучали свою модель на картинках, где лицо было в рамках картинки, поэтому, если мы хотим  сделать свою модель применимой к любому изображению человека, нам нужно решить проблему детектирования лица."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape  = [IMG_SIZE, IMG_SIZE, IMG_CHANNELS]\ninput_shape","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"[120, 120, 3]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"color_label = {0:(0,0,255),1:(0,255,0),2:(255,0,0)}\nimage = cv2.imread('../input/faces-with-masks/faces_with_mask/1006.jpg')\ngray = cv2.cvtColor(image, cv2.IMREAD_GRAYSCALE)\nfaces = face_model.detectMultiScale(gray,1.1,4) \nout_img = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n\nfor i in range(len(faces)):\n    (x,y,w,h) = faces[i]\n    crop = out_img[y:y+h,x:x+w]\n    crop = cv2.resize(crop,(IMG_SIZE,IMG_SIZE))\n    crop = np.reshape(crop,[1,IMG_SIZE,IMG_SIZE,IMG_CHANNELS])/255.0\n    mask_result = model.predict(crop)\n    if mask_result>0.5:\n        label_Y = 1\n    else:\n        label_Y = 0\n    cv2.rectangle(out_img,(x,y),(x+w,y+h),(color_label[0]),2)\n    cv2.putText(out_img,mask_label[label_Y] , (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (color_label[1]), 2)\nplt.figure(figsize=(10,10))\nplt.imshow(out_img)","execution_count":5,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"in user code:\n\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:274 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer sequential_1: expected shape=(None, 124, 124, 3), found shape=(None, 120, 120, 3)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-277577965d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_CHANNELS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmask_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask_result\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlabel_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:274 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer sequential_1: expected shape=(None, 124, 124, 3), found shape=(None, 120, 120, 3)\n"]}]},{"metadata":{},"cell_type":"markdown","source":"### Для решения этой проблемы есть много разных вариантов. Мы начнем с самого простого и нересурсоемкого: OpenCV haar cascade."},{"metadata":{},"cell_type":"markdown","source":"### Он неплохо справляется с фронтальными лицами (есть разные модули)."},{"metadata":{},"cell_type":"markdown","source":"### Создаем алгоритм, который находит лица на картинке и отмечаем их синим квадратом."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimage = cv2.imread('../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/0011.jpg')\ngray = cv2.cvtColor(image, cv2.IMREAD_GRAYSCALE)\nface_model = cv2.CascadeClassifier('../input/haar-cascades-for-face-detection/haarcascade_frontalface_default.xml')\nfaces = face_model.detectMultiScale(gray) #returns a list of (x,y,w,h) tuples\nout_img = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #colored output image\n\n#plotting\nfor (x,y,w,h) in faces:\n    cv2.rectangle(out_img,(x,y),(x+w,y+h),(color_label[0]), 2)   # blue  \nplt.figure(figsize=(10,10))\nplt.imshow(out_img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Добавим рассчет расстояния между людьми на фото (MIN_DISTANCE) и оценку соблюдения этого условия: если дистанция соблюдена -  рамка зеленая, если нет - красная.Если лиц меньше 2, то выводится сообщение, что мало лиц.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"MIN_DISTANCE = 220\n\nif len(faces)>=2:\n    label = [0 for i in range(len(faces))]\n    for i in range(len(faces)-1):\n        for j in range(i+1, len(faces)):\n            dist = distance.euclidean(faces[i][:2],faces[j][:2])\n            if dist<MIN_DISTANCE:\n                label[i] = 1\n                label[j] = 1\n\n    for i in range(len(faces)):\n        (x,y,w,h) = faces[i]\n        if label[i]==1:\n            cv2.rectangle(out_img,(x,y),(x+w,y+h),(color_label[2]),2) # red - if bad\n        else:\n            cv2.rectangle(out_img,(x,y),(x+w,y+h),(color_label[1]),2) #green - if good\n    plt.figure(figsize=(10,10))\n    plt.imshow(out_img)\n            \nelse:\n    print(\"No. of faces detected is less than 2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Добавляем нашу обученную модель и predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"if len(faces)>=2:\n    label = [0 for i in range(len(faces))]\n    for i in range(len(faces)-1):\n        for j in range(i+1, len(faces)):\n            dist = distance.euclidean(faces[i][:2],faces[j][:2])\n            if dist<MIN_DISTANCE:\n                label[i] = 1\n                label[j] = 1\n\n    for i in range(len(faces)):\n        (x,y,w,h) = faces[i]\n        crop = out_img[y:y+h,x:x+w]\n        crop = cv2.resize(crop,(IMG_SIZE,IMG_SIZE))\n        crop = np.reshape(crop,[1,IMG_SIZE,IMG_SIZE,IMG_CHANNELS])/255.0\n        mask_result = model.predict(crop)                     \n        if mask_result > 0.5:\n            label_Y = 1\n        if mask_result < 0.5:\n            label_Y = 0\n        cv2.putText(out_img,mask_label[(label_Y)] , (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,0.5,dist_label[label[i]],2)\n        cv2.rectangle(out_img,(x,y),(x+w,y+h),dist_label[label[i]],2)\n    plt.figure(figsize = (10,10))\n    plt.imshow(out_img)\n            \nelse:\n    print(\"No. of faces detected is less than 2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(faces))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Проблема: не всегда Open CV haarCascade точно определяет все лица на фото (точность -+80%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"image = cv2.imread('../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/0051.jpg')\ngray = cv2.cvtColor(image, cv2.IMREAD_GRAYSCALE)\nface_model = cv2.CascadeClassifier('../input/haar-cascades-for-face-detection/haarcascade_frontalface_default.xml')\nfaces = face_model.detectMultiScale(gray) \nout_img = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n\nif len(faces)>=2:\n    label = [0 for i in range(len(faces))]\n    for i in range(len(faces)-1):\n        for j in range(i+1, len(faces)):\n            dist = distance.euclidean(faces[i][:2],faces[j][:2])\n            if dist<MIN_DISTANCE:\n                label[i] = 1\n                label[j] = 1\n                \n    for i in range(len(faces)):\n        (x,y,w,h) = faces[i]\n        crop = out_img[y:y+h,x:x+w]\n        crop = cv2.resize(crop,(IMG_SIZE,IMG_SIZE))\n        crop = np.reshape(crop,[1,IMG_SIZE,IMG_SIZE,IMG_CHANNELS])/255.0\n        mask_result = model.predict(crop) \n        \n        if mask_result > 0.5:\n            label_Y = 1\n        if mask_result < 0.5:\n            label_Y = 0\n        cv2.putText(out_img, mask_label[(label_Y)] , (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,0.5,dist_label[label[i]],2) \n        cv2.rectangle(out_img,(x,y),(x+w,y+h),dist_label[label[i]],2)\n        \n    plt.figure(figsize=(10,10))\n    plt.imshow(out_img) \n    \nelse:\n    print(\"No. of faces detected is less than 2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### На фото толпы проблема детекции лица стоит особо остро у haar cascade:"},{"metadata":{"trusted":true},"cell_type":"code","source":"image = cv2.imread(\"../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/0025.jpg\")\ngray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY) \nfaces = face_model.detectMultiScale(gray) \nout_img = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n\nfor (x,y,w,h) in faces:\n    cv2.rectangle(out_img,(x,y),(x+w,y+h),(0,0,255),2)\nplt.figure(figsize=(8,8))\nplt.imshow(out_img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Попробуем  вариант с MTCNN (предобученной нейронной сетью) для детектирования лиц."},{"metadata":{},"cell_type":"markdown","source":"### MTCNN очень полезна, так как может работать в режиме реального времени даже на небольших устройствах. После MTCNN было много алгоритмов,но он остается одним из лучших для обнаружения лиц (frontalface  лучше всего), с гораздо лучшей точностью, чем  Open CV (haar cascade)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# draw an image with detected objects\ndef draw_image_with_boxes(img, result_list):\n    # load the image\n    data = pyplot.imread(img)\n    # plot the image\n    pyplot.imshow(data)\n    # get the context for drawing boxes\n    ax = pyplot.gca()\n    # plot each box\n    for result in result_list:\n        # get coordinates\n        x, y, width, height = result['box']\n        # create the shape\n        rect = Rectangle((x, y), width, height, fill=False, color='blue')\n        # draw the box\n        ax.add_patch(rect)\n    # show the plot\n    pyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/0025.jpg'\nimage = pyplot.imread(img)\ndetector = MTCNN()\n# detect faces in the image\nfaces = detector.detect_faces(image)\n# display faces on the original image\ndraw_image_with_boxes(img, faces)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# вот такую информацию MTCNN выводит по каждому лицу, достаточно подробно:\nfaces","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 'box' - выводит координаты лица, 'confidence' - уверенность в том, что это лицо."},{"metadata":{},"cell_type":"markdown","source":"### Как видим, результат детекции лиц гораздо лучше!"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = '../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/0044.jpg'\n# load image from file\nimage = pyplot.imread(img)\n# create the detector, using default weights\ndetector = MTCNN()\n# detect faces in the image\nfaces = detector.detect_faces(image)\ndraw_image_with_boxes(img, faces)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### А вот так MTCNN работает в связке с нашей моделью: сперва детектируются лица, затем измеряется расстояние между лицами (если Min расстояние не соблюдено - то рамки таких лиц красные, если Norm - то зеленые), затем анализируется каждое лицо через нашу обученную модель на предмет наличия маски на лице."},{"metadata":{"trusted":true},"cell_type":"code","source":"MIN_DISTANCE = 70\nimage = pyplot.imread('../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/0051.jpg')\ndetector = MTCNN()\nfaces = detector.detect_faces(image)\nout_img = image \n\nif len(faces)>=2:\n    label = [0 for i in range(len(faces))]\n    for i in range(len(faces)-1):\n        for j in range(i+1, len(faces)):\n            dist = distance.euclidean(faces[i][\"box\"][:2],faces[j][\"box\"][:2])\n            if dist<MIN_DISTANCE:\n                label[i] = 1\n                label[j] = 1\n\n    for i in range(len(faces)):\n        (x,y,w,h) = faces[i][\"box\"]\n        crop = out_img[y:y+h,x:x+w]\n        crop = cv2.resize(crop,(IMG_SIZE,IMG_SIZE))\n        crop = np.reshape(crop,[1,IMG_SIZE,IMG_SIZE,IMG_CHANNELS])/255.0\n        mask_result = model.predict(crop)        \n        if mask_result > 0.5:\n            label_Y = 1\n        elif mask_result < 0.5:\n            label_Y = 0\n        cv2.putText(out_img,mask_label[(label_Y)] , (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,0.5,dist_label[label[i]],2)\n        cv2.rectangle(out_img,(x,y),(x+w,y+h),dist_label[label[i]],2)        \nelse:\n    print(\"No. of faces detected is less than 2\")\nplt.figure(figsize=(12,12))\nplt.imshow(out_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"faces[2][\"box\"] #координаты лица №2. Такие координаты мы и будем в дальнейшем использовать.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Другой вариант оформления вывода информации после анализа:"},{"metadata":{"trusted":true},"cell_type":"code","source":"image = pyplot.imread('../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/0051.jpg')\ndetector = MTCNN()\n\nfaces = detector.detect_faces(image)\nout_img = image\nplt.figure(figsize=(15,15)) \n\nif len(faces)>=2:\n    label = [0 for i in range(len(faces))]\n    for i in range(len(faces)-1):\n        for j in range(i+1, len(faces)):\n            dist = distance.euclidean(faces[i][\"box\"][:2],faces[j][\"box\"][:2])\n            if dist<MIN_DISTANCE:\n                label[i] = 1\n                label[j] = 1    \n\n    for i in range(len(faces)):\n        (x,y,w,h) = faces[i][\"box\"]\n        crop = out_img[y:y+h,x:x+w]\n        crop = cv2.resize(crop,(IMG_SIZE,IMG_SIZE))\n        crop = np.reshape(crop,[1,IMG_SIZE,IMG_SIZE,IMG_CHANNELS])/255.0\n        mask_result = model.predict(crop) \n      \n        if mask_result > 0.5:\n            label_Y = 1\n        elif mask_result < 0.5:\n            label_Y = 0        \n        cv2.rectangle(out_img,(x,y),(x+w,y+h),dist_label[label[i]],3) \n        cv2.rectangle(out_img,(x,y-30),(x+w,y),dist_label[label[i]],-1)\n        cv2.putText(out_img, mask_label[(label_Y)], (x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.7,(255,255,255),2)\nelse:\n    print(\"No. of faces detected is less than 2\")            \nplt.imshow(out_img)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Изменим  код, чтобы он был применим и для 1 человека в кадре."},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = '../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/0015.jpg'\nimage = pyplot.imread(filename)\ndetector = MTCNN()\nfaces = detector.detect_faces(image)\nout_img = image\n\nif len(faces)>=2:\n    label = [0 for i in range(len(faces))]\n    for i in range(len(faces)-1):\n        for j in range(i+1, len(faces)):\n            dist = distance.euclidean(faces[i][\"box\"][:2],faces[j][\"box\"][:2])\n            if dist<MIN_DISTANCE:\n                label[i] = 1\n                label[j] = 1    \n\n    for i in range(len(faces)):\n        (x,y,w,h) = faces[i][\"box\"]\n        crop = out_img[y:y+h,x:x+w]\n        crop = cv2.resize(crop,(IMG_SIZE,IMG_SIZE))\n        crop = np.reshape(crop,[1,IMG_SIZE,IMG_SIZE,IMG_CHANNELS])/255.0\n        mask_result = model.predict(crop) \n                          \n        if mask_result > 0.5:\n            label_Y = 1\n        elif mask_result < 0.5:\n            label_Y = 0        \n        cv2.rectangle(out_img,(x,y),(x+w,y+h),dist_label[label[i]],3) \n        cv2.rectangle(out_img,(x,y-30),(x+w,y),dist_label[label[i]],-1)\n        cv2.putText(out_img, mask_label[(label_Y)], (x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.7,(255,255,255),2)\n\nelse:\n    for i in range(len(faces)):\n        (x,y,w,h) = faces[i][\"box\"]\n        crop = out_img[y:y+h,x:x+w]\n        crop = cv2.resize(crop,(IMG_SIZE,IMG_SIZE))\n        crop = np.reshape(crop,[1,IMG_SIZE,IMG_SIZE,IMG_CHANNELS])/255.0\n        mask_result = model.predict(crop) \n         \n        if mask_result > 0.55:\n            cv2.rectangle(out_img,(x,y),(x+w,y+h),(0,255,0),3)\n            cv2.rectangle(out_img,(x,y-50),(x+w,y),(0,255,0),-1)\n            cv2.putText(out_img,mask_label[1],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)     \n        if mask_result < 0.45:\n            cv2.rectangle(out_img,(x,y),(x+w,y+h),(255,0,0),3)\n            cv2.rectangle(out_img,(x,y-50),(x+w,y),(255,0,0),-1)\n            cv2.putText(out_img,mask_label[0],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)     \n        \npyplot.imshow(out_img) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" pyplot.imshow(out_img[y:y+h,x:x+w]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Создадим финальный код для работы с фото для вывода всей нужной нам информации: время, кол-во лиц в кадре, предупреждение, если людей больше 3+ (Х человек), анализ лиц на наличие масок, а также анализ на соблюдение социальной дистанции между людьми:"},{"metadata":{"trusted":true},"cell_type":"code","source":"image = pyplot.imread('../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images/0007.jpg')\ndetector = MTCNN()\nfaces = detector.detect_faces(image)\nout_img = image\nplt.figure(figsize=(15,15)) \ntime_now = datetime.datetime.now().replace(microsecond=0)\ncv2.putText(out_img,str(time_now),(100,70),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),2)\ncv2.putText(out_img, f\"People: {len(faces)}\",(100,110),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,0),2)  \n\nif len(faces)>=2:\n    label = [0 for i in range(len(faces))]\n    for i in range(len(faces)-1):\n        for j in range(i+1, len(faces)):\n            dist = distance.euclidean(faces[i][\"box\"][:2],faces[j][\"box\"][:2])\n            if dist<MIN_DISTANCE:\n                label[i] = 1\n                label[j] = 1    \n\n    for i in range(len(faces)):\n        (x,y,w,h) = faces[i][\"box\"]\n        crop = out_img[y:y+h,x:x+w]\n        crop = cv2.resize(crop,(IMG_SIZE,IMG_SIZE))\n        crop = np.reshape(crop,[1,IMG_SIZE,IMG_SIZE,IMG_CHANNELS])/255.0\n        mask_result = model.predict(crop) \n                          \n        if mask_result > 0.5:\n            label_Y = 1\n        elif mask_result < 0.5:\n            label_Y = 0        \n        cv2.rectangle(out_img,(x,y),(x+w,y+h),dist_label[label[i]],3) \n        cv2.rectangle(out_img,(x,y-30),(x+w,y),dist_label[label[i]],-1)\n        cv2.putText(out_img, mask_label[(label_Y)], (x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.7,(255,255,255),2)           \n\nelse:\n    for i in range(len(faces)):\n        (x,y,w,h) = faces[i][\"box\"]\n        crop = out_img[y:y+h,x:x+w]\n        crop = cv2.resize(crop,(IMG_SIZE,IMG_SIZE))\n        crop = np.reshape(crop,[1,IMG_SIZE,IMG_SIZE,IMG_CHANNELS])/255.0\n        mask_result = model.predict(crop) \n         \n        if mask_result > 0.55:\n            cv2.rectangle(out_img,(x,y),(x+w,y+h),(0,255,0),3)\n            cv2.rectangle(out_img,(x,y-50),(x+w,y),(0,255,0),-1)\n            cv2.putText(out_img,mask_label[1],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)     \n        if mask_result < 0.45:\n            cv2.rectangle(out_img,(x,y),(x+w,y+h),(255,0,0),3)\n            cv2.rectangle(out_img,(x,y-50),(x+w,y),(255,0,0),-1)\n            cv2.putText(out_img,mask_label[0],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)      \n\n# Display alarm if number of people 3+:\nif len(faces)>=3:\n    cv2.putText(out_img,str(\"Alarm: 3+ people\"),(100,30),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2)           \n            \nplt.imshow(out_img) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### В vol.3 мы продолжим усовершенствовать наш алгоритм для работы с видео потоком."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}